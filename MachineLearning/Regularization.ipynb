{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "    training data의 예측정확도는 높으나 unseen data에 대해서는 정확도가 낮으면 예측의 의미가 없음\n",
    "    학습데이터에 대해서 오차가 어느정도 있더라도 test data에 대해서 예측 정확도가 높은 모델이 더 좋은 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adressing Overfitting\n",
    "    feature의 개수를 줄이는 방법\n",
    "    변수의 개수를 줄이면 그만큼 가중치도 사라지므로 underfitting 쪽으로\n",
    "    -> 사전에 feature에 대한 판단하면 추후에 예측성능이 낮아질 수 있음(나중에 필요한 변수였음을 알 수도 있고, 필요없는 변수를 추가했을 수도 있어서)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "    loss function에 모든 가중치 각각을 제곱한거에 람다 값을 곱해준 값(regularization term)의 의미는 cost를 줄이는 수단으로 특정 가중치를 너무 높게 하면서 에러를 줄이는 거라면 그렇게는 하지 않겠다이다-> overfitting 되므로\n",
    "    regularization term이 커지면 앞의 loss function이 cost를 줄인 것을 상쇄시키기 때문\n",
    "    ex) 세타3의 optimal한 값이 7이라고 했을 때 regularization term에 의해 람다*7^2을 더하게 되면 전체 loss는 올라가므로 원래 세타3의 optimal한 값인 7보다는 작게 써야하므로 전체적으로 가중치를 낮출 수 있다.\n",
    "    한마디로 regularization term을 더했으므로 세타 자체를 전체적으로 낮게 잡아야 cost function이 커지지 않게되니까 각 세타들이 작은 cost function을 얻을 수 있다.\n",
    "    가중치가 전체적으로 크다면 data에 대해서 너무 좌지우지 됨. 그것을 방지하고자 가중치를 작게 하는 것.\n",
    "\n",
    "    람다를 너무 크게 잡으면 세타들을 0에 수렴하게 잡아야하므로 세타0만 살아남으면 그래프는 y절편만 있는 0차식이 됨 -> underfitting이 될 수도\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent in Regularization\n",
    "    세타 값이 양수일 때 regularizaiton term 이 들어간 loss function을 미분하고 원래 세타값에서 learning rate* 미분한 lossfunction의 값을 빼므로 양수인 세타 - 양수인 값이 되므로 세타값은 0에 가까워지고, 세타값이 음수일 때는 음수인 세타 - 음수인 값 이 되므로 마찬가지로 세타는 0에 가까워진다. 따라서 세타 값인 가중치 값들이 전체적으로 작아지게 되는 것이다\n",
    "\n",
    "    위의 각각 세타값들의 제곱을 더한 regularization을 L2 regularization이라고 하는데,\n",
    "    각각 세타값들의 절대값들을 더한 regularization을 L1 regularization이라고 한다.\n",
    "    L2 regularization을 하면 gradient descent에서 람다*세타의차수*세타 만큼 원래 세타에서 빼줘야하므로 세타값에 비례해서 0으로 가려고 하겠지만\n",
    "    L1 regularization에서는 절대값 세타를 미분하면 세타>0 일때는 람다만큼, 세타<0 일때는 -람다만큼 원래 세타에서 빼줘야하므로 세타값에 상관없이 일정 값인 람다만큼 0으로 가려고 할 것이다.\n",
    "    L2 regularization에서는 세타가 크다면 0으로 가려는 힘이 크겠지만, 세타가 작다면 0으로 가려는 힘이 작다. 따라서 gradient descent를 점차 진행하면할수록 세타값은 작아지기 때문에 0으로 가려는 힘이작아지므로 정확이 0이되는 세타값은 존재하지 않게 된다.\n",
    "    L1 regularization은 세타값에 관계없이 람다만큼 0으로 이동하려고 하기 때문에 0이 되는 세타값이 존재하게 된다 -> 변수를 선별할 수 있게 한다 -> sparse solution\n",
    "    L2 는 세타 값에 비례하는 만큼 세타를 0으로 끌어당기므로 outlier을 잡는데 용이하다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a5abe4f29a809acdc506f603fbc296f951feb9e596b98a49148553f8c145496"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
